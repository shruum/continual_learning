{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e52ee82-e7e5-4bb9-a1a8-efc6e009c98f",
   "metadata": {},
   "source": [
    "## Predicting input class from descriptions of higly activating images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e29c7be9-8a6a-44dd-89cb-877dffe8d4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#virtually move to parent directory\n",
    "os.chdir(\"..\")\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import clip\n",
    "import utils\n",
    "import data_utils\n",
    "import similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e693e37-a3ae-4aad-a03d-492e2a08eb5d",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1724590a-2333-4daa-9948-6be1dfc60c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_name = 'resnet50'\n",
    "target_layer = 'layer4'\n",
    "\n",
    "clip_name = 'ViT-B/16'\n",
    "d_probe = 'imagenet_broden'\n",
    "concept_set = 'data/20k.txt'\n",
    "batch_size = 200\n",
    "device = 'cuda'\n",
    "pool_mode = 'avg'\n",
    "\n",
    "save_dir = 'saved_activations'\n",
    "similarity_fn = similarity.soft_wpmi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f1eb6d-87cc-4430-b8e4-cd48d2643c7d",
   "metadata": {},
   "source": [
    "## Run CLIP-Dissect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b6a1e91-5363-43a3-8f0b-4a034515923e",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.save_activations(clip_name = clip_name, target_name = target_name, target_layers = [target_layer], \n",
    "                       d_probe = d_probe, concept_set = concept_set, batch_size = batch_size, \n",
    "                       device = device, pool_mode=pool_mode, save_dir = save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edd0e205-0b81-4d59-80ac-16b321e56949",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2048/2048 [00:14<00:00, 145.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2048, 20000])\n"
     ]
    }
   ],
   "source": [
    "save_names = utils.get_save_names(clip_name = clip_name, target_name = target_name,\n",
    "                                  target_layer = target_layer, d_probe = d_probe,\n",
    "                                  concept_set = concept_set, pool_mode=pool_mode,\n",
    "                                  save_dir = save_dir)\n",
    "\n",
    "target_save_name, clip_save_name, text_save_name = save_names\n",
    "\n",
    "similarities, target_feats = utils.get_similarity_from_activations(target_save_name, clip_save_name, \n",
    "                                                             text_save_name, similarity_fn, device=device)\n",
    "\n",
    "with open(concept_set, 'r') as f: \n",
    "    words = (f.read()).split('\\n')\n",
    "    \n",
    "vals, ids = torch.max(similarities, dim=1)\n",
    "descriptions = {\"CLIP-Dissect\":[words[int(idx)] for idx in ids]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac805683-52a8-4c01-8ca6-0fabbaf19434",
   "metadata": {},
   "source": [
    "## Calculate standard accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ede1fcd1-8f41-4f99-926b-1fdd92e4f7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#only use imagenet val for this part\n",
    "pil_data = data_utils.get_data('imagenet_val')\n",
    "target_model, target_preprocess = data_utils.get_target_model(target_name, device)\n",
    "\n",
    "save_names = utils.get_save_names(clip_name = clip_name, target_name = target_name,\n",
    "                                  target_layer = target_layer, d_probe = 'imagenet_val',\n",
    "                                  concept_set = concept_set, pool_mode=pool_mode,\n",
    "                                  save_dir = save_dir)\n",
    "target_save_name, clip_save_name, text_save_name = save_names\n",
    "\n",
    "dataset = data_utils.get_data('imagenet_val', target_preprocess)\n",
    "utils.save_target_activations(target_model, dataset, target_save_name, target_layers = [target_layer], batch_size = batch_size,\n",
    "                              device = device, pool_mode = pool_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6792ebc-46cf-4f13-9a00-b62da0b1292e",
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = torch.load(target_save_name, map_location='cpu')\n",
    "W_f = target_model.fc.weight\n",
    "b_f = target_model.fc.bias\n",
    "\n",
    "targets = torch.LongTensor(pil_data.targets).to(device)\n",
    "with open('data/imagenet_labels.txt', 'r') as f:\n",
    "    classes = f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eac48766-e06e-4b4b-bfed-d91452739e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Accuracy:76.13%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for i in range(math.ceil(len(targets)/batch_size)):\n",
    "        targ = targets[i*batch_size:(i+1)*batch_size]\n",
    "        act = activations[i*batch_size:(i+1)*batch_size].to(device)\n",
    "        out = act@W_f.T + b_f\n",
    "        pred = torch.max(out, dim=1)[1]\n",
    "        correct += torch.sum(pred==targ)\n",
    "print(\"Standard Accuracy:{:.2f}%\".format(correct/len(targets)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b02d9dc-14e9-4868-994e-bcb777f692a9",
   "metadata": {},
   "source": [
    "## Measure how often most contributing neuron description matches target class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8121976-9234-414c-957d-b97ef41adc04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 512]) torch.Size([1000, 768])\n"
     ]
    }
   ],
   "source": [
    "mpnet_model = SentenceTransformer('all-mpnet-base-v2')\n",
    "clip_model, _ = clip.load(clip_name, device=device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    tokens = clip.tokenize(classes).to(device)\n",
    "    class_clip = clip_model.encode_text(tokens)\n",
    "    class_clip /= class_clip.norm(dim=-1, keepdim=True)\n",
    "\n",
    "class_mpnet = mpnet_model.encode(classes)\n",
    "class_mpnet = torch.tensor(class_mpnet).to(device)\n",
    "print(class_clip.shape, class_mpnet.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4cdedb99-435f-45f1-a7ff-12489988123a",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_conversion = {'resnet50':'resnet50_imagenet', 'resnet18_places':'resnet18_places365'}\n",
    "\n",
    "netdissect_res = pd.read_csv('data/NetDissect_results/{}_{}.csv'.format(name_conversion[target_name],\n",
    "                                                                       target_layer))\n",
    "descriptions[\"Network Dissection\"] = netdissect_res['label'].values\n",
    "\n",
    "milan_base = pd.read_csv('data/MILAN_results/m_base_{}.csv'.format(name_conversion[target_name]))\n",
    "milan_base = milan_base[milan_base['layer']==target_layer]\n",
    "milan_base = milan_base.sort_values(by=['unit'])\n",
    "descriptions[\"MILAN base\"] = list(milan_base['description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19ccf797-3ff5-44c1-8982-c1a6f1e8531d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP-Dissect\n",
      "Same as gt:9.87%\n",
      "Same as pred:11.83% \n",
      "\n",
      "Network Dissection\n",
      "Same as gt:3.04%\n",
      "Same as pred:3.68% \n",
      "\n",
      "MILAN base\n",
      "Same as gt:2.30%\n",
      "Same as pred:2.63% \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key in descriptions:\n",
    "    print(key)\n",
    "    with torch.no_grad():\n",
    "        tokens = clip.tokenize(descriptions[key]).to(device)\n",
    "        desc_clip = clip_model.encode_text(tokens)\n",
    "        desc_clip /= desc_clip.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    desc_mpnet = mpnet_model.encode(descriptions[key])\n",
    "    desc_mpnet = torch.tensor(desc_mpnet).to(device)\n",
    "\n",
    "    correct_gt = 0\n",
    "    correct_pred = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(math.ceil(len(targets)/batch_size)):\n",
    "            targ = targets[i*batch_size:(i+1)*batch_size]\n",
    "            act = activations[i*batch_size:(i+1)*batch_size].to(device)\n",
    "\n",
    "            out = act@W_f.T + b_f\n",
    "            pred = torch.max(out, dim=1)[1]\n",
    "\n",
    "            contrib = W_f[pred]*act\n",
    "            max_contrib = torch.max(contrib, dim=1)[1]\n",
    "\n",
    "            clip_cos = desc_clip[max_contrib]@class_clip.T\n",
    "            mpnet_cos = desc_mpnet[max_contrib]@class_mpnet.T\n",
    "            \n",
    "            cos = 3*clip_cos.detach() + mpnet_cos\n",
    "            most_sim = torch.max(cos, dim=1)[1]\n",
    "            \n",
    "            correct_gt += torch.sum(most_sim==targ)\n",
    "            correct_pred += torch.sum(most_sim==pred)\n",
    "\n",
    "    print(\"Same as gt:{:.2f}%\".format(100*correct_gt/len(targets)))\n",
    "    print(\"Same as pred:{:.2f}% \\n\".format(100*correct_pred/len(targets)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:jovyan-clip_dissect]",
   "language": "python",
   "name": "conda-env-jovyan-clip_dissect-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
